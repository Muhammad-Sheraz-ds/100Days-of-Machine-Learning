{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92e1fc50-8ba7-4204-8d1a-93c7d4c3944b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.44\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 121\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_train\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Assuming you have some testing data X_test\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Make predictions on the test data\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m y_pred_test \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mX_test\u001b[49m)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Print predictions on the test data\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions on Test Data:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_pred_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, C=1.0, tol=1e-3, max_iter=100):\n",
    "        # SVM hyperparameters\n",
    "        self.C = C  # Regularization parameter\n",
    "        self.tol = tol  # Tolerance for numerical stability\n",
    "        self.max_iter = max_iter  # Maximum number of iterations for the SMO algorithm\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Training the SVM using the Sequential Minimal Optimization (SMO) algorithm\n",
    "        self.X = X  # Training data\n",
    "        self.y = y  # Labels\n",
    "        self.alpha = np.zeros(len(X))  # Lagrange multipliers\n",
    "        self.b = 0.0  # Intercept term\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            num_changed_alphas = 0\n",
    "\n",
    "            # Iterate over all training examples\n",
    "            for i in range(len(X)):\n",
    "                # Compute the decision function value for the current example\n",
    "                E_i = self.decision_function(X[i]) - y[i]\n",
    "\n",
    "                # Check if the example violates the KKT conditions\n",
    "                if ((y[i] * E_i < -self.tol and self.alpha[i] < self.C) or\n",
    "                        (y[i] * E_i > self.tol and self.alpha[i] > 0)):\n",
    "                    # Select a second example j different from i\n",
    "                    j = self.select_random_index(i)\n",
    "                    E_j = self.decision_function(X[j]) - y[j]\n",
    "\n",
    "                    # Save old alpha values for later comparison\n",
    "                    alpha_i_old, alpha_j_old = self.alpha[i], self.alpha[j]\n",
    "\n",
    "                    # Compute the bounds for alpha[j] to satisfy KKT conditions\n",
    "                    if y[i] != y[j]:\n",
    "                        L = max(0, self.alpha[j] - self.alpha[i])\n",
    "                        H = min(self.C, self.C + self.alpha[j] - self.alpha[i])\n",
    "                    else:\n",
    "                        L = max(0, self.alpha[i] + self.alpha[j] - self.C)\n",
    "                        H = min(self.C, self.alpha[i] + self.alpha[j])\n",
    "\n",
    "                    # Check if the bounds are equal, continue to the next iteration\n",
    "                    if L == H:\n",
    "                        continue\n",
    "\n",
    "                    # Compute the second derivative of the objective function\n",
    "                    eta = 2 * np.dot(X[i], X[j]) - np.dot(X[i], X[i]) - np.dot(X[j], X[j])\n",
    "\n",
    "                    # If eta is non-positive, continue to the next iteration\n",
    "                    if eta >= 0:\n",
    "                        continue\n",
    "\n",
    "                    # Update alpha[j]\n",
    "                    self.alpha[j] -= (y[j] * (E_i - E_j)) / eta\n",
    "                    self.alpha[j] = np.clip(self.alpha[j], L, H)\n",
    "\n",
    "                    # If the change in alpha[j] is small, continue to the next iteration\n",
    "                    if abs(self.alpha[j] - alpha_j_old) < 1e-5:\n",
    "                        continue\n",
    "\n",
    "                    # Update alpha[i]\n",
    "                    self.alpha[i] += y[i] * y[j] * (alpha_j_old - self.alpha[j])\n",
    "\n",
    "                    # Update the intercept term\n",
    "                    b1 = self.b - E_i - y[i] * (self.alpha[i] - alpha_i_old) * np.dot(X[i], X[i]) - \\\n",
    "                         y[j] * (self.alpha[j] - alpha_j_old) * np.dot(X[i], X[j])\n",
    "                    b2 = self.b - E_j - y[i] * (self.alpha[i] - alpha_i_old) * np.dot(X[i], X[j]) - \\\n",
    "                         y[j] * (self.alpha[j] - alpha_j_old) * np.dot(X[j], X[j])\n",
    "\n",
    "                    if 0 < self.alpha[i] < self.C:\n",
    "                        self.b = b1\n",
    "                    elif 0 < self.alpha[j] < self.C:\n",
    "                        self.b = b2\n",
    "                    else:\n",
    "                        self.b = (b1 + b2) / 2\n",
    "\n",
    "                    num_changed_alphas += 1\n",
    "\n",
    "            # If no alphas were changed in this iteration, exit the loop\n",
    "            if num_changed_alphas == 0:\n",
    "                break\n",
    "\n",
    "    def decision_function(self, x):\n",
    "        # Decision function for predicting the class of a sample\n",
    "        return np.dot(self.alpha * self.y, np.dot(self.X, x)) - self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions on input data\n",
    "        return np.sign(np.array([self.decision_function(x) for x in X]))\n",
    "\n",
    "    def select_random_index(self, i):\n",
    "        # Select a random index different from the given index i\n",
    "        j = i\n",
    "        while j == i:\n",
    "            j = np.random.randint(len(self.X))\n",
    "        return j\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X_train and y_train are your training data and labels\n",
    "# X_test is your testing data\n",
    "\n",
    "# Generate some random data for testing\n",
    "np.random.seed(42)\n",
    "X_train = np.random.randn(100, 2)\n",
    "y_train = np.where(X_train[:, 0] + X_train[:, 1] > 0, 1, -1)\n",
    "\n",
    "svm = SVM(C=1.0)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training data\n",
    "y_pred_train = svm.predict(X_train)\n",
    "\n",
    "# Print accuracy on the training data\n",
    "accuracy_train = np.mean(y_pred_train == y_train)\n",
    "print(f\"Training Accuracy: {accuracy_train}\")\n",
    "\n",
    "# Assuming you have some testing data X_test\n",
    "# Make predictions on the test data\n",
    "y_pred_test = svm.predict(X_test)\n",
    "\n",
    "# Print predictions on the test data\n",
    "print(\"Predictions on Test Data:\", y_pred_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ebd3eb-32d1-4224-9435-5f8af187beab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SVM_classifier():\n",
    "\n",
    "  # Initiating the hyperparameters\n",
    "  def __init__(self, learning_rate, no_of_iterations, lambda_parameter):\n",
    "    self.learning_rate = learning_rate\n",
    "    self.no_of_iterations = no_of_iterations\n",
    "    self.lambda_parameter = lambda_parameter\n",
    "\n",
    "  # Fitting the dataset to SVM Classifier\n",
    "  def fit(self, X, Y):\n",
    "    # m  --> number of data points (rows)\n",
    "    # n  --> number of input features (columns)\n",
    "    self.m, self.n = X.shape\n",
    "\n",
    "    # Initiating the weight value and bias value\n",
    "    self.w = np.zeros(self.n)\n",
    "    self.b = 0\n",
    "    self.X = X\n",
    "    self.Y = Y\n",
    "\n",
    "    # Implementing Gradient Descent algorithm for optimization\n",
    "    for i in range(self.no_of_iterations):\n",
    "      self.update_weights()\n",
    "\n",
    "  # Function for updating the weight and bias value\n",
    "  def update_weights(self):\n",
    "    # Label encoding: Convert Y values to -1 for <= 0 and 1 for > 0\n",
    "    y_label = np.where(self.Y <= 0, -1, 1)\n",
    "\n",
    "    # Gradients (dw, db) for each data point\n",
    "    for index, x_i in enumerate(self.X):\n",
    "      # Condition for checking if the point is correctly classified\n",
    "      condition = y_label[index] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "\n",
    "      if condition:\n",
    "        # If correctly classified, update gradients without considering the data point\n",
    "        dw = 2 * self.lambda_parameter * self.w\n",
    "        db = 0\n",
    "      else:\n",
    "        # If incorrectly classified, update gradients considering the data point\n",
    "        dw = 2 * self.lambda_parameter * self.w - np.dot(x_i, y_label[index])\n",
    "        db = y_label[index]\n",
    "\n",
    "      # Update weights and bias using the gradients and learning rate\n",
    "      self.w = self.w - self.learning_rate * dw\n",
    "      self.b = self.b - self.learning_rate * db\n",
    "\n",
    "  # Predict the label for a given input value\n",
    "  def predict(self, X):\n",
    "    # Calculate the output based on the learned weights and bias\n",
    "    output = np.dot(X, self.w) - self.b\n",
    "\n",
    "    # Convert the output to predicted labels (0 or 1) using sign function\n",
    "    predicted_labels = np.sign(output)\n",
    "\n",
    "    # Convert predicted labels to 0 if <= -1, and 1 otherwise\n",
    "    y_hat = np.where(predicted_labels <= -1, 0, 1)\n",
    "\n",
    "    return y_hat\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
