{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6686c3f",
   "metadata": {},
   "source": [
    "<img src=\"images/pandas-intro.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fc3b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16a38f6b",
   "metadata": {},
   "source": [
    "# Learning Agenda of this Notebook:\n",
    "- What is Pandas and how is it used in AI?\n",
    "- Key features of Pandas\n",
    "- Data Types in Pandas\n",
    "- What does Pandas deal with?\n",
    "\n",
    "- Creating Series in Pandas\n",
    "    - From Python List\n",
    "    - From NumPy Arrays\n",
    "    - From Python Dictionary\n",
    "    - From a scalar value\n",
    "    - Creating empty series object\n",
    "- Attributes of a Pandas Series\n",
    "- Arithmetic Operations on Series\n",
    "\n",
    "- Dataframes in Pandas\n",
    "    - Anatomy of a Dataframe\n",
    "    - Creating Dataframe\n",
    "        - An empty dataframe\n",
    "        - Two-Dimensional NumPy Array\n",
    "        - Dictionary of Python Lists\n",
    "        - Dictionary of Panda Series\n",
    "    - Attributes of a Dataframe\n",
    "    - Bonus\n",
    "- Data Handling with Pandas\n",
    "  - Practice Exercise I\n",
    "  - Practice Exercise II\n",
    "- All Statistical functions in Pandas\n",
    "- Input/Output Operations\n",
    "- Aggregation & Grouping\n",
    "  - Practice Exercise\n",
    "- Merging, Joining and Concatenation\n",
    "  - Practice Exercise\n",
    "- How To Perform Data Visualization with Pandas\n",
    "- Exercise I\n",
    "- Exercise II\n",
    "- Pandas's Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff01c774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d0b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5157b310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5184b585",
   "metadata": {},
   "source": [
    "## _IO with CSV EXCEL and JSON Files_\n",
    "\n",
    "<img align=\"center\" width=\"600\" height=\"150\"  src=\"images/fileformats.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410559b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b47c92a2",
   "metadata": {},
   "source": [
    "#### Read Pandas Documentation:\n",
    "- General Info: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html\n",
    "\n",
    "\n",
    "- For `read_csv`: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html?highlight=read_csv#pandas.read_csv\n",
    "\n",
    "\n",
    "- For `read_excel`: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html?highlight=read_excel#pandas.read_excel\n",
    "\n",
    "\n",
    "- For `read_json`:https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.io.json.read_json.html?highlight=pandas%20read_json#pandas.io.json.read_json\n",
    "\n",
    "\n",
    "- For `to_csv`: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html#pandas.DataFrame.to_csv\n",
    "\n",
    "\n",
    "\n",
    "- For `to_excel`: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html?highlight=to_excel#pandas.DataFrame.to_excel\n",
    "\n",
    "\n",
    "- For `to_json`: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_json.html?highlight=to_json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd192d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44990632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc404dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b064523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35192588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f154a69f",
   "metadata": {},
   "source": [
    "## Data Handling with Pandas..\n",
    "\n",
    "- **Data Reading** : Reading from a csv or an excel – Pandas provide two functions – read_csv() and read_excel() to read data from a csv and an excel file respectively. Command can be used as follows.\n",
    "\n",
    "- **Viewing data** – Viewing data from a data frame can be done by three ways\n",
    " >- using the data frame’s name – returns the top and bottom 5 rows in the data frame.\n",
    " >- using dataframe.head() function\n",
    " >- using dataframe.tail() function\n",
    "\n",
    "- **Data Overview** : To see more details on the data frame, the `info()` function can be used. info() gives an idea about what datatype each series in a data frame points to.\n",
    "\n",
    "- The following functions are used to find the unique entries within a series/column in a data frame.\n",
    " >- datafame.unique() – returns the unique values\n",
    " >- dataframe.nunique() – returns the count of unique values\n",
    " >- dataframe.value_counts() – returns the frequency of each of the categories in the column\n",
    "\n",
    "- In our example, the titanic dataset contains a column called `Survived` which tells if the particular passenger survived the tragedy. Since this value could only be either 0 or 1, we can convert the data type from integer to object.\n",
    " >- `dataframe.astype()` is the function which lets us do the conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c04a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat datasets/titanic3.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e34a30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.listdir('datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304bf522",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/recent-grads.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec1d4f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55306a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13227ab2",
   "metadata": {},
   "source": [
    "## Practice Questions Part 1: \n",
    "- Step 1. Import the necessary libraries.\n",
    "- Step 2. Import the dataset from this [address](https://raw.githubusercontent.com/bsef19m521/DatasetsForProjects/master/u.user)\n",
    "- Step 3. Assign it to a variable called users and use the `user_id` as index\n",
    "- Step 4. See the first 25 entries\n",
    "- Step 5. See the last 10 entries\n",
    "- Step 6. What is the number of observations in the dataset?\n",
    "- Step 7. What is the number of columns in the dataset?\n",
    "- Step 8. Print the name of all the columns.\n",
    "- Step 9. How is the dataset indexed?\n",
    "- Step 10. What is the data type of each column?\n",
    "- Step 11. Print only the occupation column\n",
    "- Step 12. How many different occupations are in this dataset?\n",
    "- Step 13. What is the most frequent occupation?\n",
    "- Step 14. Summarize the DataFrame.\n",
    "- Step 15. Summarize all the columns\n",
    "- Step 16. Summarize only the occupation column\n",
    "- Step 17. What is the mean age of users?\n",
    "- Step 18. What is the age with least occurrence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf93c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# data reading\n",
    "url = \"https://raw.githubusercontent.com/bsef19m521/DatasetsForProjects/master/u.user\"\n",
    "users = pd.read_csv(url, delimiter=\"|\")\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7377b9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task no 03\n",
    "# First Method\n",
    "users = pd.read_csv(url, delimiter=\"|\", index_col='user_id')\n",
    "users.head()\n",
    "\n",
    "\n",
    "# Second method\n",
    "# users.set_index('user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918d17cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task no 04\n",
    "users.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964824a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task no 05\n",
    "users.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041382f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task no 06\n",
    "users.shape\n",
    "# or \n",
    "users.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62914be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task no 07 & 08\n",
    "print(\"users.shape : \",users.shape)\n",
    "users.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task no 09\n",
    "users.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ff59f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b04b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task no 10\n",
    "users.dtypes\n",
    " \n",
    "#     or \n",
    "# users.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5f6419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task no 11\n",
    "users['occupation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f0a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task no 12\n",
    "# First method\n",
    "users['occupation'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea5b6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second method\n",
    "users['occupation'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260e2ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third method\n",
    "users['occupation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb98e393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d8ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task no 13\n",
    "users['occupation'].value_counts()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657a7712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task no 14\n",
    "users.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b254322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task no 15\n",
    "users.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5f5d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task no 16\n",
    "users['occupation'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f38ff9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfbf5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task no 17\n",
    "users['age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588c9ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task no 18\n",
    "users['age'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ab535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# pd.Series([])\n",
    "# pd.DataFrame(dict)\n",
    "# shape\n",
    "# info() -> indcies of columns, columns name, total null values, datatype of each column, range of indcies\n",
    "# dtypes -> return datatype of all columns\n",
    "# describe() -> descriptive view dataset/dataframe (mean, max, min, std, count)\n",
    "# head() -> top records/rows/indcies\n",
    "# tail() -> last 5 records\n",
    "# unique()  -> all unique values in given column(it is invalid for continous data)\n",
    "# nunique() -> return count of unique values\n",
    "# value_counts() -> return unique values along with their frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92facbd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42a91c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aae2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb4225b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "498ee523",
   "metadata": {},
   "source": [
    "## Practice Questions Part 2:\n",
    "- Step 1. Import the necessary libraries\n",
    "- Step 2. Import the dataset from this [address](https://raw.githubusercontent.com/bsef19m521/DatasetsForProjects/master/Euro_2012_stats_TEAM.csv)\n",
    "- Step 3. Assign it to a variable called `euro12`.\n",
    "- Step 4. Select only the Goal column.\n",
    "- Step 5. How many team participated in the Euro2012?(value_counts/shape)\n",
    "- Step 6. What is the number of columns in the dataset?(shape/info)\n",
    "- Step 7. View only the columns Team, Yellow Cards and Red Cards and assign them to a dataframe called discipline\n",
    "- Step 8. Sort the teams by Red Cards, then to Yellow Cards(Hint: sort_values)\n",
    "- Step 9. Calculate the mean Yellow Cards given per Team(Hint: round())\n",
    "- Step 10. Filter teams that scored more than 6 goals\n",
    "- Step 11. Select the teams that start with G(Hint : str.startswith('G'))\n",
    "- Step 12. Select the first 7 columns and all the rows(Hint: iloc())\n",
    "- Step 13. Select all columns except the last 3.(Hint: iloc())\n",
    "- Step 14. Presents/shows only the Shooting Accuracy from England, Italy and Russia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4824ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/bsef19m521/DatasetsForProjects/master/Euro_2012_stats_TEAM.csv\"\n",
    "euro12 = pd.read_csv(url)\n",
    "euro12.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb74b28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "euro12['Goals']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b2b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "euro12['Team'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c029874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# euro12.shape\n",
    "# or \n",
    "euro12.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc0c093",
   "metadata": {},
   "outputs": [],
   "source": [
    "discipline = euro12[['Team','Yellow Cards','Red Cards']]\n",
    "discipline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3361eb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discipline = discipline.sort_values('Red Cards')\n",
    "# discipline = discipline.sort_values('Yellow Cards')\n",
    "\n",
    "\n",
    "#  Or\n",
    "\n",
    "discipline.sort_values(by =['Red Cards','Yellow Cards'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d06c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(discipline.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672c0c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "euro12.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4525432",
   "metadata": {},
   "outputs": [],
   "source": [
    "euro12['Goals']>6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ffefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "euro12[euro12['Goals'] > 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd564ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "euro12['Team'].str.startswith('G')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0377bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "euro12[euro12['Team'].str.startswith('G')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1309ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "euro12.iloc[:,:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8df27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "euro12.iloc[:,-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef8e7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a65628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # euro12.loc[), ['Team','Shooting Accuracy']]\n",
    "euro12.loc[euro12.Team.isin(['England', 'Italy', 'Russia']), ['Team','Shooting Accuracy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32b3bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0bad24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd5d730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e62cb2f",
   "metadata": {},
   "source": [
    "## All statistical functions\n",
    "- `count()` : Returns the number of times an element/data has occurred (non-null)\n",
    "- `sum()`\t: Returns sum of all values\n",
    "- `mean()` : Returns the average of all values\n",
    "- `median()` : Returns the median of all values\n",
    "- `mode()` : Returns the mode\n",
    "- `std()`\t: Returns the standard deviation\n",
    "- `min()`\t: Returns the minimum of all values\n",
    "- `max()`\t: Returns the maximum of all values\n",
    "- `abs()`\t: Returns the absolute value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33f7e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a7b4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of elements in each column of dataframe \")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336a2c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b93034",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sum(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4530e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def3c276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e84798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cdd9edb",
   "metadata": {},
   "source": [
    "## Input and Output\n",
    "\n",
    "- Often, you won’t be creating data but will be having it in some form, and you would want to import it to run your analysis on it. Fortunately, Pandas allows you to do this. Not only does it help in importing data, but you can also save your data in your desired format using Pandas.\n",
    "- The below table shows the formats supported by Pandas, the function to read files using Pandas, and the function to write files.\n",
    "|Input |type      |\tReader\tWriter |\n",
    "|------|----------|----------------|\n",
    "|CSV   |read_csv  |  to_csv        |\n",
    "|JSON  |read_json | to_json\n",
    "|HTML  |read_html |to_html\n",
    "|Excel |read_excel|to_excel\n",
    "|SAS   |read_sas  |–\n",
    "|Python|Pickle    |\tread_pickle\tto_pickle\n",
    "|SQL   |read_sql  |to_sql\n",
    "|Google|Big Query | read_gbq\tto_gbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236bf81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read input file\n",
    "df = pd.read_csv('datasets/psl.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918a2b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da389b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a dataframe to CSV File\n",
    "data = {'Name':['Captain America', 'Iron Man', 'Hulk', 'Thor','Black Panther'],\n",
    "        'Rating':[100, 80, 84, 93, 90],\n",
    "        'Place':['USA','USA','USA','Asgard','Wakanda']}\n",
    "# Create dataframe from above dictionary\n",
    "df = pd.DataFrame(data)\n",
    "df\n",
    "df.to_csv(\"datasets/avengers1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e93d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat datasets/avengers1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ab45c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effc0dec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39481c21",
   "metadata": {},
   "source": [
    "## Aggregation\n",
    "- The aggregation function can be applied against a single or more column. You can either apply the same aggregate function across various columns or different aggregate functions across various columns.\n",
    "- Syntax : \n",
    " >- DataFrame.aggregate(self, func, axis=0, *args, ***kwargs)\n",
    " \n",
    "<img src=\"images/pandas-agg-func.png\" height=400px width=600px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b6b77c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00061903",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = 'http://bit.ly/2cLzoxH'\n",
    "# read data from url as pandas dataframe\n",
    "gapminder = pd.read_csv(data_url)\n",
    "gapminder.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8d707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gapminder_data = gapminder[['continent','pop']]\n",
    "gapminder_data.head()\n",
    "# gapminder.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de645bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Aggregate Functions on Series\n",
    "mean  = gapminder_data['pop'].aggregate('mean')\n",
    "print(\"Mean of population : \", mean)\n",
    "\n",
    "Min  = gapminder_data['pop'].aggregate('min')\n",
    "print(\"Minimum value of population : \", Min)\n",
    "\n",
    "Max  = gapminder_data['pop'].aggregate('max')\n",
    "print(\"Maximum value of population : \", Max)\n",
    "\n",
    "Std  = gapminder_data['pop'].aggregate('std')\n",
    "print(\"Std of population : \", Std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ac5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a7bec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb466959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using multiple Aggregate Functions on Dataframe\n",
    "gapminder_data['pop'].aggregate(['sum','min','max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc654c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using multiple Aggregate Functions on Multiple columns of Dataframe\n",
    "gapminder[['pop','lifeExp']].aggregate(['sum','min','max','std','mean','var'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f0586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also perform above task by using below code\n",
    "gapminder.aggregate({'pop':['sum','min','max'],\n",
    "                    'lifeExp':['sum','min','max']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8818b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.describe()  gives overall descriptive view of our dataset\n",
    "gapminder.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43ffbc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08379b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "701dc508",
   "metadata": {},
   "source": [
    "## Groupby\n",
    "- Pandas groupby function is used to split the DataFrame into groups based on some criteria. \n",
    "- Similar to the `SQL GROUP BY` clause pandas `DataFrame.groupby()` function is used to collect the identical data into groups and perform aggregate functions on the grouped data. Group by operation involves splitting the data, applying some functions, and finally aggregating the results.\n",
    "\n",
    "<img src=\"images/pandas-groupby-standard-dev.png.webp\" height=500px width=500px>\n",
    "<img src=\"images/groupby-example.png\" height=700px width=700px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcc92c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38f42b61",
   "metadata": {},
   "source": [
    "### Syntax of Pandas DataFrame.groupby()\n",
    "\n",
    "       \n",
    "       `DataFrame.groupby(by=None, axis=0, level=None, as_index=True,     \n",
    "       sort=True, group_keys=True, squeeze=<no_default>,      \n",
    "       observed=False, dropna=True)`\n",
    "       \n",
    "       \n",
    "- `by` – List of column names to group by\n",
    "- `axis` – Default to 0. It takes 0 or ‘index’, 1 or ‘columns’\n",
    "- `level` – Used with MultiIndex.\n",
    "- `as_index` – sql style grouped otput.\n",
    "- `sort` – Default to True. Specify whether to sort after group\n",
    "- `group_keys` – add group keys or not\n",
    "- `squeeze` – depricated in new versions\n",
    "- `observed` – This only applies if any of the groupers are Categoricals.\n",
    "- `dropna` – Default to False. Use True to drop None/Nan on sory key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c354dcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3f0d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "technologies   = ({\n",
    "    'Courses':[\"Spark\",\"PySpark\",\"Hadoop\",\"Python\",\"Pandas\",\"Hadoop\",\"Spark\",\"Python\",np.nan],\n",
    "    'Fee' :[22000,25000,23000,24000,26000,25000,25000,22000,1500],\n",
    "    'Duration':['30days','50days','55days','40days','60days','35days','30days','50days','40days'],\n",
    "    'Discount':[1000,2300,1000,1200,2500,None,1400,1600,0]\n",
    "          })\n",
    "df = pd.DataFrame(technologies)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94a6caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf37f42d",
   "metadata": {},
   "source": [
    "#### Use groupby() to compute the sum of Fee and Discount of each course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b11bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Courses']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f22981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2\n",
    "df.groupby(['Courses'])[['Fee','Discount']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eb7106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly\n",
    "df.groupby(['Courses']).aggregate('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e7e045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa363720",
   "metadata": {},
   "source": [
    "#### pandas groupby() on Two or More Columns like Courses and Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e69a05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby(['Courses','Duration']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c215129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98b7b54",
   "metadata": {},
   "source": [
    "#### Add Index to the grouped data\n",
    "- By default `groupby()` result doesn’t include row Index, you can add the index using `DataFrame.reset_index()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65ff9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Courses','Duration']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5be5e6c",
   "metadata": {},
   "source": [
    "#### Remove sorting on grouped results by using `sort` parameter of df.groupby()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079ff937",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2=df.groupby(by=['Courses'], sort=False).sum()\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb7b15e",
   "metadata": {},
   "source": [
    "#### Apply More Aggregations\n",
    "- You can also compute several aggregations at the same time in pandas by passing the list of agg functions to the `aggregate().`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d24cf15",
   "metadata": {},
   "source": [
    "#### Compute minimu and maximum fee of each course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a67d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Courses')['Fee'].aggregate(['min','max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f17ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby multiple columns & multiple aggregations\n",
    "df.groupby('Courses').aggregate({'Duration':'count',\n",
    "                                'Fee':['min','max']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778abea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db7e4e53",
   "metadata": {},
   "source": [
    "## Practice Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1198aa13",
   "metadata": {},
   "source": [
    "### Regiment\n",
    "- A regiment is a military unit. Its role and size varies markedly, depending on the country, service and/or a specialisation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c826893b",
   "metadata": {},
   "source": [
    "#### Step 1. Import the necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9821aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297c02ba",
   "metadata": {},
   "source": [
    "#### Step 2. Create the DataFrame with the following values and Assign it to a variable called regiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28246a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {'regiment': ['Nighthawks', 'Nighthawks', 'Nighthawks', 'Nighthawks', 'Dragoons', 'Dragoons', 'Dragoons', 'Dragoons', 'Scouts', 'Scouts', 'Scouts', 'Scouts'], \n",
    "        'company': ['1st', '1st', '2nd', '2nd', '1st', '1st', '2nd', '2nd','1st', '1st', '2nd', '2nd'], \n",
    "        'name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze', 'Jacon', 'Ryaner', 'Sone', 'Sloan', 'Piger', 'Riani', 'Ali'], \n",
    "        'preTestScore': [4, 24, 31, 2, 3, 4, 24, 31, 2, 3, 2, 3],\n",
    "        'postTestScore': [25, 94, 57, 62, 70, 25, 94, 57, 62, 70, 62, 70]}\n",
    "regiment = pd.DataFrame(raw_data)\n",
    "regiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c635b963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c97a2a02",
   "metadata": {},
   "source": [
    "#### Step 3. What is the mean `preTestScore` from the regiment `Nighthawks`(Nightbird/Night owl)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c33f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Method\n",
    "regiment[regiment['regiment'] == 'Nighthawks'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fac618",
   "metadata": {},
   "outputs": [],
   "source": [
    "regiment[regiment['regiment'] == 'Nighthawks']['preTestScore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7264ab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Method\n",
    "regiment[regiment['regiment'] == 'Nighthawks']['preTestScore'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3181435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regiment[regiment['regiment'] == 'Nighthawks']\n",
    "\n",
    "# 3rd Method\n",
    "regiment.groupby('regiment').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e6611f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ae0f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "regiment.groupby('regiment').get_group('Nighthawks')['preTestScore'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71420455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d1d68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9ef1ea2",
   "metadata": {},
   "source": [
    "#### Step 4. Present/show general statistics by `company` of regiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00b90c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regiment.groupby('company').describe()\n",
    "\n",
    "regiment.groupby('company').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7103f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5df0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "489c67d0",
   "metadata": {},
   "source": [
    "#### Step 5. What is the mean of each company's preTestScore?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983e5189",
   "metadata": {},
   "outputs": [],
   "source": [
    "regiment.groupby('company')['preTestScore'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce04ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regiment.groupby('company')['preTestScore'].mean()\n",
    "\n",
    "# OR\n",
    "\n",
    "regiment.groupby('company').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf158fa",
   "metadata": {},
   "source": [
    "#### Step 6. Presents/shows the `mean` preTestScores grouped by regiment and company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0473b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "regiment.groupby(['regiment','company'])['preTestScore'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214f3cf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# regiment.groupby(['regiment','company'])['preTestScore'].mean()\n",
    "# OR\n",
    "regiment.groupby(['regiment', 'company']).preTestScore.mean().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c1f5ef",
   "metadata": {},
   "source": [
    "#### Step 7. Presents/shows the `mean` preTestScores grouped by regiment and company with reset_index parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5c9263",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regiment.groupby(['regiment', 'company'])['preTestScore'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20273f15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da845e19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6aa8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48911819",
   "metadata": {},
   "source": [
    "#### Step 8. Group the entire dataframe by regiment and company , also perform `sum` aggregate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b04db0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "regiment.groupby(['regiment','company']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5660c165",
   "metadata": {},
   "source": [
    "#### Step 9. What is the number of observations in each regiment and company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf22f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "regiment.groupby(['regiment','company']).size()\n",
    "# OR \n",
    "regiment.groupby(['regiment','company']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462657d0",
   "metadata": {},
   "source": [
    "#### Step 10. Iterate over a group and print the name and the whole data from the regiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cd8111",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, data in regiment.groupby('regiment'):\n",
    "    print(\"Group Name :\", name)\n",
    "    print(\"Group Data : \", data,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da1ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Group the dataframe by regiment, and for each regiment,\n",
    "# for name, group in regiment.groupby('regiment'):\n",
    "#     # print the name of the regiment\n",
    "#     print('Name : ',name)\n",
    "# #     print data of that regiment\n",
    "#     print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c97960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94c74048",
   "metadata": {},
   "source": [
    "## Merging, Joining and Concatenation\n",
    "Before I start with Pandas join and merge functions, let me introduce you to four different types of joins, they are inner join, left join, right join, outer join.\n",
    "<img src=\"images/Untitled.png\" height=500px width=500px align=\"right\"> \n",
    "\n",
    "- **Full outer join**: Combines results from both DataFrames. The result will have all columns from both DataFrames.\n",
    "- **Inner join**: Only those rows which are present in both DataFrame A and DataFrame B will be present in the output.\n",
    "- **Right join**: Right join uses all records from DataFrame B and matching records from DataFrame A.\n",
    "- **Left join**: Left join uses all records from DataFrame A and matching records from DataFrame B.\n",
    "\n",
    "<img src=\"images/joins.png\" height=600px width=600px align=\"left\" > \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b5ecea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ab9cb91",
   "metadata": {},
   "source": [
    "### Merging\n",
    "- Merging a Dataframe with one unique key.\n",
    "\n",
    "#### Syntax:\n",
    "```\n",
    "pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None,\n",
    "left_index=False, right_index=False, sort=True)\n",
    "``` \n",
    "- `left` − A DataFrame object.\n",
    "- `right` − Another DataFrame object.\n",
    "- `on` − Columns (names) to join on. Must be found in both the left and right DataFrame objects.\n",
    "- `left_on` − Columns from the left DataFrame to use as keys. Can either be column names or arrays with length equal to the length of the DataFrame.\n",
    "- `right_on` − Columns from the right DataFrame to use as keys. Can either be column names or arrays with length equal to the length of the DataFrame.\n",
    "- `left_index` − If True, use the index (row labels) from the left DataFrame as its join key(s). In case of a DataFrame with a MultiIndex (hierarchical), the number of levels must match the number of join keys from the right DataFrame.\n",
    "- `right_index` − Same usage as left_index for the right DataFrame.\n",
    "- `how` − One of 'left', 'right', 'outer', 'inner'. Defaults to inner. Each method has been described below.\n",
    "- `sort` − Sort the result DataFrame by the join keys in lexicographical order. Defaults to True, setting to False will improve the performance substantially in many cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74b251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary containing employee data \n",
    "import pandas as pd\n",
    "data1 = {'key':['K0','K1','K2','K3'],\n",
    "         'Name':['Mercy', 'Prince', 'John', 'Cena'],\n",
    "         'Age':[27, 24, 22, 32],} \n",
    "# Define a dictionary containing employee data \n",
    "\n",
    "data2 = {'key':['K0','K1','K2','K3'],\n",
    "         'Address':['Canada', 'UK', 'India', 'USA'], \n",
    "         'Qualification':['Btech', 'B.A', 'MS', 'Phd']} \n",
    "\n",
    "# Convert the dictionary into DataFrame  \n",
    "df1 = pd.DataFrame(data1)\n",
    "# Convert the dictionary into DataFrame  \n",
    "df2 = pd.DataFrame(data2) \n",
    "\n",
    "df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9f46ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging of two dataframes on basis ok `key` \n",
    "final_df = pd.merge(df1, df2, on='key')\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdeb316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc3d47dd",
   "metadata": {},
   "source": [
    "#### Merging Dataframe using multiple keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a5d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary containing employee data \n",
    "\n",
    "data1 = {'key':['K0','K1','K2','K3'],\n",
    "         'Name':['Mercy', 'Prince', 'John', 'Cena'],\n",
    "          'Address':['Canada', 'Australia', 'India', 'Japan'],\n",
    "         'Age':[27, 24, 22, 32],} \n",
    "# Define a dictionary containing employee data \n",
    "\n",
    "data2 = {'key':['K0','K1','K2','K3'],\n",
    "         'Address':['Canada', 'UK', 'India', 'USA'], \n",
    "         'Qualification':['Btech', 'B.A', 'MS', 'Phd']} \n",
    "\n",
    "# Convert the dictionary into DataFrame  \n",
    "df1 = pd.DataFrame(data1)\n",
    "# Convert the dictionary into DataFrame  \n",
    "df2 = pd.DataFrame(data2) \n",
    "\n",
    "df1.Address, df2.Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53ae03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging of two dataframes on basis ok `key` \n",
    "final_df = pd.merge(df1, df2, on=['key','Address'])\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8403ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11c4a4e7",
   "metadata": {},
   "source": [
    "#### Left merge\n",
    "- In pd.merge() I pass the argument `how = left` to perform a left merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da43cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary containing employee data \n",
    "\n",
    "data1 = {'key':['K0','K1','K2','K3'],\n",
    "         'Name':['Mercy', 'Prince', 'John', 'Cena'],\n",
    "          'Address':['Canada', 'Australia', 'India', 'Japan'],\n",
    "         'Age':[27, 24, 22, 32],} \n",
    "# Define a dictionary containing employee data \n",
    "\n",
    "data2 = {'key':['K0','K1','K2','K3'],\n",
    "         'Address':['Canada', 'UK', 'India', 'USA'], \n",
    "         'Qualification':['Btech', 'B.A', 'MS', 'Phd']} \n",
    "\n",
    "# Convert the dictionary into DataFrame  \n",
    "df1 = pd.DataFrame(data1)\n",
    "# Convert the dictionary into DataFrame  \n",
    "df2 = pd.DataFrame(data2) \n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e3e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5630fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging of two dataframes on basis ok `key` \n",
    "final_df = pd.merge(df1, df2, on=['key','Address'], how='left')\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477530d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9aba6b4",
   "metadata": {},
   "source": [
    "#### Right merge\n",
    "- In pd.merge() I pass the argument `how = right` to perform a left merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db97cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary containing employee data \n",
    "\n",
    "data1 = {'key':['K0','K1','K2','K3'],\n",
    "         'Name':['Mercy', 'Prince', 'John', 'Cena'],\n",
    "          'Address':['Canada', 'Australia', 'India', 'Japan'],\n",
    "         'Age':[27, 24, 22, 32],} \n",
    "# Define a dictionary containing employee data \n",
    "\n",
    "data2 = {'key':['K0','K1','K2','K3'],\n",
    "         'Address':['Canada', 'UK', 'India', 'USA'], \n",
    "         'Qualification':['Btech', 'B.A', 'MS', 'Phd']} \n",
    "\n",
    "# Convert the dictionary into DataFrame  \n",
    "df1 = pd.DataFrame(data1)\n",
    "# Convert the dictionary into DataFrame  \n",
    "df2 = pd.DataFrame(data2) \n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d507079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90e2a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# merging of two dataframes on basis ok `key` \n",
    "final_df = pd.merge(df1, df2, on=['key','Address'], how='right')\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cd03bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17b7a4dc",
   "metadata": {},
   "source": [
    "#### Outer Merge\n",
    "- In pd.merge(), I pass the argument `how = outer` to perform a outer merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed3d5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary containing employee data \n",
    "\n",
    "data1 = {'key':['K0','K1','K2','K3'],\n",
    "         'Name':['Mercy', 'Prince', 'John', 'Cena'],\n",
    "          'Address':['Canada', 'Australia', 'India', 'Japan'],\n",
    "         'Age':[27, 24, 22, 32],} \n",
    "# Define a dictionary containing employee data \n",
    "\n",
    "data2 = {'key':['K0','K1','K2','K3'],\n",
    "         'Address':['Canada', 'UK', 'India', 'USA'], \n",
    "         'Qualification':['Btech', 'B.A', 'MS', 'Phd']} \n",
    "\n",
    "# Convert the dictionary into DataFrame  \n",
    "df1 = pd.DataFrame(data1)\n",
    "# Convert the dictionary into DataFrame  \n",
    "df2 = pd.DataFrame(data2) \n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658229b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b324a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# merging of two dataframes on basis ok `key` \n",
    "final_df = pd.merge(df1, df2, on=['key','Address'], how='outer')\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ca1ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd34723f",
   "metadata": {},
   "source": [
    "## Join\n",
    "- Join is used to combine DataFrames having different indcies values.\n",
    "- `I have two different tables in Python but I’m not sure how to join them. What criteria should I consider? What are the different ways I can join these tables?`\n",
    "- Sound familiar? I have come across this question plenty of times on online discussion forums. Working with one table is fairly straightforward but things become challenging when we have data spread across two or more tables.\n",
    "- This is where the concept of Joins comes in. I cannot emphasize the number of times I have used these Joins in Pandas! They’ve come in especially handy during data science hackathons when I needed to quickly join multiple tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82c9017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7fa0d44",
   "metadata": {},
   "source": [
    "#### Understanding the Problem Statement\n",
    "\n",
    "- I’m sure you’re quite familiar with e-commerce sites like `Amazon` and `Flipkart` these days. We are bombarded by their advertisements when we’re visiting non-related websites – that’s the power of targeted marketing!\n",
    "- We’ll take a simple problem from a related marketing brand here. We are given two tables – one which contains data about products and the other that has customer-level information.\n",
    "- We will use these tables to understand how the different types of joins work using Pandas.\n",
    "\n",
    "#### Note: \n",
    " >- Our task is to use our joining skills and generate meaningful information from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afa26b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The product dataframe contains product details like Product_ID, Product_name, Category, Price, and Seller_City. \n",
    "product=pd.DataFrame({\n",
    "    'Product_ID':[101,102,103,104,105,106,107],\n",
    "    'Product_name':['Watch','Bag','Shoes','Smartphone','Books','Oil','Laptop'],\n",
    "    'Category':['Fashion','Fashion','Fashion','Electronics','Study','Grocery','Electronics'],\n",
    "    'Price':[299.0,1350.50,2999.0,14999.0,145.0,110.0,79999.0],\n",
    "    'Seller_City':['Delhi','Mumbai','Chennai','Kolkata','Delhi','Chennai','Bengalore']\n",
    "})\n",
    "\n",
    "# The customer dataframe contains details like id, name, age, Product_ID, Purchased_Product, and City.\n",
    "customer=pd.DataFrame({\n",
    "    'id':[1,2,3,4,5,6,7,8,9],\n",
    "    'name':['Olivia','Aditya','Cory','Isabell','Dominic','Tyler','Samuel','Daniel','Jeremy'],\n",
    "    'age':[20,25,15,10,30,65,35,18,23],\n",
    "    'Product_ID':[101,0,106,0,103,104,0,0,107],\n",
    "    'Purchased_Product':['Watch','NA','Oil','NA','Shoes','Smartphone','NA','NA','Laptop'],\n",
    "    'City':['Mumbai','Delhi','Bangalore','Chennai','Chennai','Delhi','Kolkata','Delhi','Mumbai']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0181585b",
   "metadata": {},
   "source": [
    "- Let’s say we want to know about all the products sold online and who purchased them. We can get this easily using an inner join.\n",
    "\n",
    "- The `merge()` function in Pandas is our friend here. By default, the merge function performs an inner join. It takes both the dataframes as arguments and the name of the column on which the join has to be performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db600c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d486fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644f7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46dc7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(product, customer, on='Product_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df78879",
   "metadata": {},
   "source": [
    "- Here, I have performed inner join on the product and customer dataframes on the `Product_ID` column.\n",
    "- But, what if the column names are different in the two dataframes? Then, we have to explicitly mention both the column names.\n",
    "- `left_on` and `right_on` are two arguments through which we can achieve this. `left_on` is the name of the key in the left dataframe and `right_on` in the right dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ca9ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(product, customer, left_on='Product_name', right_on='Purchased_Product')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ba408d",
   "metadata": {},
   "source": [
    "- Let’s take things up a notice. The leadership team now wants more details about the products sold. They want to know about all the products sold by the seller to the same city i.e., seller and customer both belong to the same city.\n",
    "\n",
    "- In this case, we have to perform an inner join on both Product_ID and Seller_City of product and Product_ID and City columns of the customer dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c6d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(product, customer, left_on=['Product_ID', 'Seller_City'], right_on=['Product_ID','City'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcfb3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3f077b2",
   "metadata": {},
   "source": [
    "## Concatenation\n",
    "Concatenating of two or more dataframes using `.concat()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc0b9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First DataFrame : \", df1, sep=\"\\n\")\n",
    "print(\"Second DataFrame : \", df2, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c171ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df1, df2]\n",
    "# concatenation using concate function\n",
    "pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e53080e",
   "metadata": {},
   "source": [
    "The resultant DataFrame has a repeated index. If you want the new Dataframe to have its own index, set `ignore_index` to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f981c077",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df1, df2]\n",
    "# concatenation using concate function\n",
    "pd.concat(frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa980038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f789d00",
   "metadata": {},
   "source": [
    "#### Note: \n",
    " >- The second DataFrame is concatenating below the first one, making the resultant DataFrame have new rows. If you want the second DataFrame to be added as columns, pass the argument axis=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e5446",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df1, df2]\n",
    "# concatenation using concate function\n",
    "pd.concat(frames, axis=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68cdc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(frames, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c623e2",
   "metadata": {},
   "source": [
    "#### Note: \n",
    " >- Here columns of resultant dataframes are repeated to avoid this, we will append() function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b67637",
   "metadata": {},
   "source": [
    "### Concatenating using `.append()` function\n",
    "- Append function concatenates along axis = 0 only. It can take multiple objects as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bab0f8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1.append(df2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf409a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6462fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9119ab0b",
   "metadata": {},
   "source": [
    "## Practices \n",
    "- Import pandas library.\n",
    "- Download both datasets for this exercise from here [data1](https://raw.githubusercontent.com/bsef19m521/DatasetsForProjects/master/data1.csv) and [data2](https://raw.githubusercontent.com/bsef19m521/DatasetsForProjects/master/data2.csv) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a238e939",
   "metadata": {},
   "source": [
    "### Step 1 : Write a program to join the two given dataframes along rows and assign all to variable `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb9ea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('datasets/data1.csv')\n",
    "data2 = pd.read_csv('datasets/data2.csv')\n",
    "print(\"First Data : \", data1, sep=\"\\n\")\n",
    "print(\"Second Data : \", data2, sep=\"\\n\")\n",
    "print(\"Joining of two dataframes along rows wise ...\")\n",
    "data = pd.concat([data1, data2],)\n",
    "data\n",
    "\n",
    "# pd.concat([df1,df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecad0c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c5eed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "890ff8e3",
   "metadata": {},
   "source": [
    "### Step 2 : Write a program to join the two given dataframes along columns and assign all to variable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41ab952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 = pd.read_csv('datasets/data1.csv')\n",
    "# data2 = pd.read_csv('datasets/data2.csv')\n",
    "# # print(\"First Data : \", data1, sep=\"\\n\")\n",
    "# # print(\"Second Data : \", data2, sep=\"\\n\")\n",
    "# print(\"Joining of two dataframes along rows wise ...\")\n",
    "data = pd.concat([data1, data2],axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3463bcd0",
   "metadata": {},
   "source": [
    "### Step 3 : Write a Pandas program to append rows to an existing DataFrame `data1` and display the combined data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb43b353",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series(['S6','Ehtisham Sadiq', 187], index=['student_id', 'name', 'marks'])\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9458b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = data1.append(s1, ignore_index=True)\n",
    "print(\"Combined data : \", combined_data, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75560e1e",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "- For `pandas.DataFrame`, both `join` and `merge` operates on columns and rename the common columns using the given suffix. In terms of row-wise alignment, `merge` provides more flexible control.\n",
    "- Different from `join` and `merge`, `concat` can operate on columns or rows, depending on the given axis, and no renaming is performed. In addition, `concat` allows defining hierachy structures by passing in `keys` and `names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c8a6ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3033a5a8",
   "metadata": {},
   "source": [
    "## How To Perform Data Visualization with Pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb06db2b",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "- Data visualization is the most important step in the life cycle of data science, data analytics, or we can say in data engineering. It is more impressive, interesting and understanding when we represent our study or analysis with the help of colours and graphics. Using visualization elements like graphs, charts, maps, etc., it becomes easier for clients to understand the underlying structure, trends, patterns and relationships among variables within the dataset. Simply explaining the data summary and analysis using plain numbers becomes complicated for both, people coming from technical and non-technical backgrounds. Data visualization gives us a clear idea of what the data wants to convey to us. It makes data neutral for us to understand the data insights.\n",
    "- Data visualization involves operating a huge amount of data and converts it into meaningful and knowledgeable visuals using various tools. For visualizing data we need the best software tools to handle various types of data in structured or unstructured format from different sources such as files, web API, databases, and many more. We must choose the best visualization tool that fulfils all our requirements. The tool should support interactive plots generation, connectivity to data sources, combining data sources, automatically refresh the data, secured access to data sources, and exporting widgets. All these features allow us to make the best visuals of our data and also save time.\n",
    "#### Advantages of Data Visualization\n",
    "<img src=\"images/70513benefits.jpg\" height=600px width=600px >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a54ec3a",
   "metadata": {},
   "source": [
    "### Data Visualization with Pandas:\n",
    "\n",
    "- Pandas library in python is mainly used for data analysis. It is not a data visualization library but, we can create basic plots using Pandas. Pandas is highly useful and practical if we want to create exploratory data analysis plots. We do not need to import other data visualization libraries in addition to Pandas for such tasks.\n",
    "\n",
    "- As Pandas is Python’s popular data analysis library, it provides several different functions to visualizing our data with the help of the .plot() function. There is one more advantage of using Pandas for visualization is we can serialize or create a pipeline of data analysis functions and plotting functions. It simplifies the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adba7660",
   "metadata": {},
   "source": [
    "#### Creating of Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504dcfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series(np.arange(1,21))\n",
    "y = x**1.2\n",
    "z = x**1.7\n",
    "w = x**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421fed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = {'col1':x,\n",
    "        'col2':y,\n",
    "        'col3':z,\n",
    "        'col4':w}\n",
    "# dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a152fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#creating a DataFrame\n",
    "df = pd.DataFrame(dict1)\n",
    "# Since this is a randomly generated dataframe the values will differ everytime you run this code for everyone.\n",
    "\n",
    "#displaying the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8eccdc",
   "metadata": {},
   "source": [
    "#### Line plot:\n",
    "- Line plot can be created with DataFrame.plot() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2deb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4827ac78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f4058cc",
   "metadata": {},
   "source": [
    "We have got the well-versed line plot for `df` without specifying any type of features in the `.plot()` function. We can plot graphs between two columns also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26aac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x='col1', y='col4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd4c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['col3','col4']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c3b524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also generate subplots for individual columns.\n",
    "df.plot(subplots=True, figsize=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3def5d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faf189fb",
   "metadata": {},
   "source": [
    "### Bar plot:\n",
    "- Now, we will create bar plots for the same dataframe. Bar plot can be created with `DataFrame.plot.bar()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a97272",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac952420",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.bar(stacked=True)\n",
    "# In this bar plot, the bars are stacked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dc833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.barh(stacked=True)\n",
    "# In this bar plot, the bars are stacked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abc32ec",
   "metadata": {},
   "source": [
    "### Histogram Plot:\n",
    "Now, let’s generate a histogram for the `df`. Histogram plot can be created with `DataFrame.plot.hist()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9017ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let’s create a histogram with some other features.\n",
    "df.plot.hist(stacked=True, bins=15)\n",
    "# This is a stacked histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fa6a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.hist(orientation=\"horizontal\", cumulative=True);\n",
    "# Here, we have added a cumulative frequency in the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24b35b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s create a histogram for each column individually.\n",
    "df.diff().hist(bins=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da81a6c3",
   "metadata": {},
   "source": [
    "### Box Plot:\n",
    "Now, we will create box plot. Box plot can be created with `DataFrame.plot.box()` function or `DataFrame.boxplot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65979c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Method    \n",
    "df.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fef876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Method\n",
    "df.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef443b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, generating the box plot in a horizontal form.\n",
    "df.plot.box(vert=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0738cf1b",
   "metadata": {},
   "source": [
    "### Area plot:\n",
    "Now, we will create a area plot. Area plot can be created with `DataFrame.plot.area()` function. By default, it is stacked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac3117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.area()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2915a7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will create unstacked area plot.\n",
    "df.plot.area(stacked=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f598cc28",
   "metadata": {},
   "source": [
    "### Scatter plot:\n",
    "Now, let’s generate a scatter plot. A Scatter plot can be created with `DataFrame.plot.scatter()` function. As we know scatter plot takes two-positional required arguments i.e. x and y to plot the graph. So, we will give the values of the  `x-axis` and `y-axis` as the name of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee8071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter('col4', 'col3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f595ae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the scatter plot between col_1 and col_2 of dataframe df. Let’s apply some styles.\n",
    "ax = df.plot.scatter('col1', 'col2', color='r', marker=\"*\", s=100)\n",
    "df.plot.scatter(x='col3',y='col4', color='b', s=100, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e532913",
   "metadata": {},
   "source": [
    "In this plot the data is spread with respect to col_2 and col_4 and the we have added some styles also like color, marker  and size of scatters. Let’s see another style of scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b05c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x='col2', y='col4', c='col1', s=100)\n",
    "# The c keyword is given as the name of a column to provide colours for each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892d80c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9452e6ce",
   "metadata": {},
   "source": [
    "### Pie chart:\n",
    "- A Pie plot can be created with `DataFrame.plot.pie()` function or `Series.plot.pie()`. To generate a pie chart we will create series data as a pie chart is created only for one column. Let’s create a series named pie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4a7c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pie = pd.Series(np.random.randint(10,100,4))\n",
    "pie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916650e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pie.plot.pie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f5e7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's apply some styles\n",
    "pie.plot.pie(autopct='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d13f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fda6177",
   "metadata": {},
   "source": [
    "### Pie Chart for DataFrame\n",
    "- A Pie chart can be created for DataFrames  also but it will generate individual pies for each column of DataFrame in the form of subplots. Let’s Create a pie chart for the dataframe also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd656a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(np.random.randint(20,100,(5,3)),columns=['col1','col2','col3'])\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04270a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.plot.pie(subplots=True, figsize=(15,15), autopct='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333389c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3631148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c925141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e230f0a",
   "metadata": {},
   "source": [
    "## Practice Exercise Part 1:\n",
    "#### Visualizing the Titanic Disaster\n",
    "- Step 1. Import the necessary libraries\n",
    "- Step 2. Import the dataset from this [address](https://raw.githubusercontent.com/bsef19m521/DatasetsForProjects/master/train.csv)\n",
    "- Step 3. Assign it to a variable titanic\n",
    "- Step 4. Set PassengerId as the index\n",
    "- Step 5. Create a pie chart presenting the male/female proportion\n",
    "- Step 6. Create a scatterplot with the Fare payed and the Age, differ the plot color by gender\n",
    "- Step 7. How many people survived and died , display using pie chart?\n",
    "- Step 8. Create a histogram with the Fare payed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315342a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://raw.githubusercontent.com/bsef19m521/DatasetsForProjects/master/train.csv\"\n",
    "# titanic = pd.read_csv(url,)\n",
    "\n",
    "# # titanic = pd.read_csv(url,index_col='PassengerId')\n",
    "# # titanic.head()\n",
    "# # OR\n",
    "# titanic.set_index('PassengerId').head()\n",
    "# titanic.shape\n",
    "# males = (titanic.Sex == 'male').sum()\n",
    "# females = (titanic.Sex =='female').sum()\n",
    "# print(\"Total Males : \", males)\n",
    "# print(\"Total Females : \", females)\n",
    "# new_titanic = pd.Series([males,females])\n",
    "# new_titanic\n",
    "# new_titanic.plot.pie(labels=['Male','Female'], autopct='%.2f')\n",
    "# titanic.columns\n",
    "# # titanic.head()\n",
    "# list1 = []\n",
    "# for i in titanic.Sex:\n",
    "#     if i=='male':\n",
    "#         list1.append(1)\n",
    "#     else:\n",
    "#         list1.append(0)\n",
    "# titanic['new_Sex'] = list1\n",
    "# titanic.head()\n",
    "# titanic.plot.scatter(x='Fare',y='Age',c='new_Sex',s=10)\n",
    "# titanic.Fare.min(), titanic.Fare.max(), titanic.Fare.mean()\n",
    "# titanic.Fare.plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93668be6",
   "metadata": {},
   "source": [
    "### Practice Exercise Part 2:\n",
    "- Step 1. Import the necessary libraries\n",
    "- Step 2. Import the dataset given below\n",
    "- Step 3. Assign it to a variable `df3`\n",
    "- Step 4. Create a scatter plot of `b` vs `a` by using `red` color.\n",
    "- Step 5. Create a histogram of the `a` column.\n",
    "- Step 6. Create a histogram of the `b` column and use bins=30.\n",
    "- Step 7. Create a boxplot comparing the `a` and `b` columns.\n",
    "- Step 8. Create a kde plot of the `d` column.\n",
    "- Step 9. Create a kde plot of the `d` column and Figure out how to increase the linewidth and make the linestyle dashed. (Note: You would usually not dash a kde plot line)\n",
    "- Step 10. Create an area plot of all the columns for just the rows up to 30. (hint: alpha=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8ced04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3 = pd.DataFrame(np.random.rand(500,4), columns=['a','b','c','d'])\n",
    "# df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ea5673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62784ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28de8d05",
   "metadata": {},
   "source": [
    "## Basic Python Pandas Exercise \n",
    "- In this exercise, we are using `Automobile Dataset` for data analysis. This Dataset has different characteristics of an auto such as body-style, wheel-base, engine-type, price, mileage, horsepower, etc.\n",
    "- Download dataset from this link [Automobile data_set](https://raw.githubusercontent.com/bsef19m521/DatasetsForProjects/master/Automobile_data.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b838d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "url= \"https://raw.githubusercontent.com/bsef19m521/DatasetsForProjects/master/Automobile_data.csv\"\n",
    "automobile = pd.read_csv(url)\n",
    "automobile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634d4f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "automobile.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154a4b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "automobile.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a7bf28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abad6b5e",
   "metadata": {},
   "source": [
    "### Exercise 1: From the given dataset print the first and last five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcfe4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "automobile.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d28a131",
   "metadata": {},
   "outputs": [],
   "source": [
    "automobile.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2437bfec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03fd1b4e",
   "metadata": {},
   "source": [
    "### Exercise 2: Clean the dataset and update the CSV file(Hint: pd.read_csv(na_values={})\n",
    "Replace all column values which contain `?`, `n.a`, or `NaN.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39057c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "automobile = pd.read_csv(url, na_values={'?':np.nan,\n",
    "                                        'n.a':np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e15b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "automobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b3aeff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f163efa",
   "metadata": {},
   "source": [
    "### Exercise 3: Find the most expensive car company name\n",
    "Print most expensive car’s company name and price.     \n",
    "**Expected Output:**\n",
    "![](images/pandas_printing_most_costly_car_name.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e63d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = automobile.groupby('company')[['price']].max().sort_values(by='price').tail(1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce044e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36a2299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Method\n",
    "a = automobile.groupby(['company'])['price'].max()\n",
    "a.sort_values(ascending=False).reset_index().head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6419cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d75fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Method\n",
    "b = automobile[['company','price']][automobile['price'] == automobile['price'].max()]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1267b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "automobile[['company','price']][automobile.price == automobile.price.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8a8637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ba999a8",
   "metadata": {},
   "source": [
    "### Exercise 4: Print All Toyota Cars details\n",
    "**Expected Output**\n",
    "![](images/pandas_printing_all_toyota_car_data.png)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8516cb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automobile[automobile.company == 'toyota']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15550206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First Method\n",
    "# automobile[automobile['company'] == 'toyota']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a631ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Second Method\n",
    "group = automobile.groupby('company')\n",
    "group.get_group('toyota')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b76d874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb5c98e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a54f69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c14f2da",
   "metadata": {},
   "source": [
    "### Exercise 5: Count total cars per company\n",
    "**Expected Output**\n",
    "![](images/pandas_count_total_cars_per_company.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29634f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "automobile.company.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a09ae4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfb28b24",
   "metadata": {},
   "source": [
    "### Exercise 6: Find each company’s Higesht price car\n",
    "**Expected Outcome:**\n",
    "![](images/pandas_printing_each_companys_higesht_price_car.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2824cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Method\n",
    "automobile.groupby(['company'])['price'].max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6e19f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Method\n",
    "automobile.groupby('company')[['company','price']].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3b36d7",
   "metadata": {},
   "source": [
    "### Exercise 7: Find the average mileage of each car making company\n",
    "**Expected Output:**\n",
    "![](images/pandas_printing_average_mileage_of_each_car_making_company.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467aadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Method\n",
    "automobile.groupby('company')['company','average-mileage'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df38afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Method\n",
    "result = automobile.groupby('company')\n",
    "result['company','average-mileage'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd090654",
   "metadata": {},
   "source": [
    "### Exercise 8: Sort all cars by Price column\n",
    "**Expected Output:**\n",
    "![](images/pandas_sort_all_cars_by_price_column.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b178e899",
   "metadata": {},
   "outputs": [],
   "source": [
    "automobile.sort_values(by=['price'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc19be4",
   "metadata": {},
   "source": [
    "### Exercise 9: Concatenate two data frames using the following conditions\n",
    "Create two data frames using the following two dictionaries.\n",
    "![](images/pandas_concatenate_two_data_frames_and_create_key_for_each_data_frame.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280ea9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "GermanCars = {'Company': ['Ford', 'Mercedes', 'BMV', 'Audi'], 'Price': [23845, 171995, 135925 , 71400]}\n",
    "japaneseCars = {'Company': ['Toyota', 'Honda', 'Nissan', 'Mitsubishi '], 'Price': [29995, 23600, 61500 , 58900]}\n",
    "German = pd.DataFrame(GermanCars)\n",
    "Japan = pd.DataFrame(japaneseCars)\n",
    "df = pd.concat([German,Japan], keys=['German','Japan'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0987546c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea5e8f3f",
   "metadata": {},
   "source": [
    "### Exercise 10: Merge two data frames using the following condition\n",
    "Create two data frames using the following two Dicts, Merge two data frames, and append the second data frame as a new column to the first data frame.\n",
    "![](images/merge_two_data_frames_and_append_new_data_frame_as_new-column.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed369803",
   "metadata": {},
   "outputs": [],
   "source": [
    "Car_Price = {'Company': ['Toyota', 'Honda', 'BMV', 'Audi'], 'Price': [23845, 17995, 135925 , 71400]}\n",
    "car_Horsepower = {'Company': ['Toyota', 'Honda', 'BMV', 'Audi'], 'horsepower': [141, 80, 182 , 160]}\n",
    "price = pd.DataFrame.from_dict(Car_Price)\n",
    "horsepower = pd.DataFrame.from_dict(car_Horsepower)\n",
    "pd.merge(price,horsepower,on='Company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29ba3af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fd3124d",
   "metadata": {},
   "source": [
    "## Pandas Data Visualization Exercise\n",
    "This is just a quick exercise for you to review the various plots we showed earlier. Use **datasets/practice.csv** to replicate the following plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9ab777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfb16d8",
   "metadata": {},
   "source": [
    "### Q-01: Import your dataset and also display first five rows of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8e99ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/practice')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77e0fb6",
   "metadata": {},
   "source": [
    "**Q-02: Create this scatter plot of `b` vs `a`. Note the color and size of the points. Also note the figure size. See if you can figure out how to stretch it in a similar fashion. Remeber back to your matplotlib lecture...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e30a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x='b',y='a', color='r', s=100, figsize=(10,5), title=\"Scatter plot of b vs a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdee4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56c3f111",
   "metadata": {},
   "source": [
    "**Create a histogram of the 'a' column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe1f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.plot.hist(by= 'a', bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7afe968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a79e90fc",
   "metadata": {},
   "source": [
    "**These plots are okay, but they don't look very polished. Use style sheets to set the style to 'plt.style.use('ggplot') and redo the histogram from above. Also figure out how to add more `bins` and `alpha` to it.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2e67e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-dark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89918ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c04396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e11537c7",
   "metadata": {},
   "source": [
    "**Create a boxplot comparing the `a` and `b` columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f25485",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['a','b']].plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e016065a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3563a20",
   "metadata": {},
   "source": [
    "**Create a kde plot of the `d` column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d18c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot('d', kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9157fba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53452bb8",
   "metadata": {},
   "source": [
    "**Figure out how to increase the linewidth and make the linestyle dashed. (Note: You would usually not dash a kde plot line)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e1169f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c50a7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "195e90c3",
   "metadata": {},
   "source": [
    "**Create an area plot of all the columns for just the rows up to `30.` (hint: use `.ix`).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68358a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0f5e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2577f2a",
   "metadata": {},
   "source": [
    "# Pandas - Assignment No 01\n",
    "- Click here to solve [Pandas - Assignment no 01](https://www.kaggle.com/code/ehtishamsadiq/pandas-assignment-no-01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5084fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f768ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9953560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d092076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f380359b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f2c4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef6df26d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        body {\n",
       "            background-color: #f2fff2;\n",
       "        }\n",
       "        h1 {\n",
       "            text-align: center;\n",
       "            font-weight: bold;\n",
       "            font-size: 36px;\n",
       "            color: #4295F4;\n",
       "            text-decoration: underline;\n",
       "            padding-top: 15px;\n",
       "        }\n",
       "        \n",
       "        h2 {\n",
       "            text-align: left;\n",
       "            font-weight: bold;\n",
       "            font-size: 30px;\n",
       "            color: #4A000A;\n",
       "            text-decoration: underline;\n",
       "            padding-top: 10px;\n",
       "        }\n",
       "        \n",
       "        h3 {\n",
       "            text-align: left;\n",
       "            font-weight: bold;\n",
       "            font-size: 30px;\n",
       "            color: #f0081e;\n",
       "            text-decoration: underline;\n",
       "            padding-top: 5px;\n",
       "        }\n",
       "\n",
       "        \n",
       "        p {\n",
       "            text-align: center;\n",
       "            font-size: 12 px;\n",
       "            color: #0B9923;\n",
       "        }\n",
       "    </style>\n",
       "\n",
       "<h1>Hello</h1>\n",
       "<p>Hello World</p>\n",
       "<h2> Hello</h2>\n",
       "<h3> World </h3>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "style = \"\"\"\n",
    "    <style>\n",
    "        body {\n",
    "            background-color: #f2fff2;\n",
    "        }\n",
    "        h1 {\n",
    "            text-align: center;\n",
    "            font-weight: bold;\n",
    "            font-size: 36px;\n",
    "            color: #4295F4;\n",
    "            text-decoration: underline;\n",
    "            padding-top: 15px;\n",
    "        }\n",
    "        \n",
    "        h2 {\n",
    "            text-align: left;\n",
    "            font-weight: bold;\n",
    "            font-size: 30px;\n",
    "            color: #4A000A;\n",
    "            text-decoration: underline;\n",
    "            padding-top: 10px;\n",
    "        }\n",
    "        \n",
    "        h3 {\n",
    "            text-align: left;\n",
    "            font-weight: bold;\n",
    "            font-size: 30px;\n",
    "            color: #f0081e;\n",
    "            text-decoration: underline;\n",
    "            padding-top: 5px;\n",
    "        }\n",
    "\n",
    "        \n",
    "        p {\n",
    "            text-align: center;\n",
    "            font-size: 12 px;\n",
    "            color: #0B9923;\n",
    "        }\n",
    "    </style>\n",
    "\"\"\"\n",
    "\n",
    "html_content = \"\"\"\n",
    "<h1>Hello</h1>\n",
    "<p>Hello World</p>\n",
    "<h2> Hello</h2>\n",
    "<h3> World </h3>\n",
    "\"\"\"\n",
    "\n",
    "HTML(style + html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2c8316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
